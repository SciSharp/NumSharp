// Copyright (c) 2019 by the SciSharp Team
// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Python.Included;
using Numpy.Models;

namespace Numpy
{
    public partial class NumPy
    {
        
        /// <summary>
        /// Dot product of two arrays. Specifically,
        /// </summary>
        /// <param name="a">
        /// First argument.
        /// </param>
        /// <param name="b">
        /// Second argument.
        /// </param>
        /// <param name="@out">
        /// Output argument. This must have the exact kind that would be returned
        /// if it was not used. In particular, it must have the right type, must be
        /// C-contiguous, and its dtype must be the dtype that would be returned
        /// for dot(a,b). This is a performance feature. Therefore, if these
        /// conditions are not met, an exception is raised, instead of attempting
        /// to be flexible.
        /// </param>
        /// <returns>
        /// Returns the dot product of a and b.  If a and b are both
        /// scalars or both 1-D arrays then a scalar is returned; otherwise
        /// an array is returned.
        /// If out is given, then it is returned.
        /// </returns>
        public NDarray dot(NDarray a, NDarray b, NDarray @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                a,
                b,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("dot", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Compute the dot product of two or more arrays in a single function call,
        /// while automatically selecting the fastest evaluation order.
        /// 
        /// multi_dot chains numpy.dot and uses optimal parenthesization
        /// of the matrices [1] [2]. Depending on the shapes of the matrices,
        /// this can speed up the multiplication a lot.
        /// 
        /// If the first argument is 1-D it is treated as a row vector.
        /// If the last argument is 1-D it is treated as a column vector.
        /// The other arguments must be 2-D.
        /// 
        /// Think of multi_dot as:
        /// 
        /// Notes
        /// 
        /// The cost for a matrix multiplication can be calculated with the
        /// following function:
        /// 
        /// Let’s assume we have three matrices
        /// .
        /// 
        /// The costs for the two different parenthesizations are as follows:
        /// 
        /// References
        /// </summary>
        /// <param name="arrays">
        /// If the first argument is 1-D it is treated as row vector.
        /// If the last argument is 1-D it is treated as column vector.
        /// The other arguments must be 2-D.
        /// </param>
        /// <returns>
        /// Returns the dot product of the supplied arrays.
        /// </returns>
        public NDarray multi_dot(NDarray[] arrays)
        {
            //auto-generated code, do not change
            var linalg = self.GetAttr("linalg");
            var __self__=linalg;
            var pyargs=ToTuple(new object[]
            {
                arrays,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("multi_dot", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Return the dot product of two vectors.
        /// 
        /// The vdot(a, b) function handles complex numbers differently than
        /// dot(a, b).  If the first argument is complex the complex conjugate
        /// of the first argument is used for the calculation of the dot product.
        /// 
        /// Note that vdot handles multidimensional arrays differently than dot:
        /// it does not perform a matrix product, but flattens input arguments
        /// to 1-D vectors first. Consequently, it should only be used for vectors.
        /// </summary>
        /// <param name="a">
        /// If a is complex the complex conjugate is taken before calculation
        /// of the dot product.
        /// </param>
        /// <param name="b">
        /// Second argument to the dot product.
        /// </param>
        /// <returns>
        /// Dot product of a and b.  Can be an int, float, or
        /// complex depending on the types of a and b.
        /// </returns>
        public NDarray vdot(NDarray a, NDarray b)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                a,
                b,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("vdot", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Inner product of two arrays.
        /// 
        /// Ordinary inner product of vectors for 1-D arrays (without complex
        /// conjugation), in higher dimensions a sum product over the last axes.
        /// 
        /// Notes
        /// 
        /// For vectors (1-D arrays) it computes the ordinary inner-product:
        /// 
        /// More generally, if ndim(a) = r &gt; 0 and ndim(b) = s &gt; 0:
        /// 
        /// or explicitly:
        /// 
        /// In addition a or b may be scalars, in which case:
        /// </summary>
        /// <param name="b">
        /// If a and b are nonscalar, their last dimensions must match.
        /// </param>
        /// <param name="a">
        /// If a and b are nonscalar, their last dimensions must match.
        /// </param>
        /// <returns>
        /// out.shape = a.shape[:-1] + b.shape[:-1]
        /// </returns>
        public NDarray inner(NDarray b, NDarray a)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                b,
                a,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("inner", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Compute the outer product of two vectors.
        /// 
        /// Given two vectors, a = [a0, a1, ..., aM] and
        /// b = [b0, b1, ..., bN],
        /// the outer product [1] is:
        /// 
        /// References
        /// </summary>
        /// <param name="a">
        /// First input vector.  Input is flattened if
        /// not already 1-dimensional.
        /// </param>
        /// <param name="b">
        /// Second input vector.  Input is flattened if
        /// not already 1-dimensional.
        /// </param>
        /// <param name="@out">
        /// A location where the result is stored
        /// </param>
        /// <returns>
        /// out[i, j] = a[i] * b[j]
        /// </returns>
        public NDarray outer(NDarray a, NDarray b, NDarray @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                a,
                b,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("outer", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Matrix product of two arrays.
        /// 
        /// Notes
        /// 
        /// The behavior depends on the arguments in the following way.
        /// 
        /// matmul differs from dot in two important ways:
        /// 
        /// The matmul function implements the semantics of the &#64; operator introduced
        /// in Python 3.5 following PEP465.
        /// </summary>
        /// <param name="x2">
        /// Input arrays, scalars not allowed.
        /// </param>
        /// <param name="x1">
        /// Input arrays, scalars not allowed.
        /// </param>
        /// <param name="@out">
        /// A location into which the result is stored. If provided, it must have
        /// a shape that matches the signature (n,k),(k,m)-&gt;(n,m). If not
        /// provided or None, a freshly-allocated array is returned.
        /// </param>
        /// <returns>
        /// The matrix product of the inputs.
        /// This is a scalar only when both x1, x2 are 1-d vectors.
        /// </returns>
        public NDarray matmul(NDarray x2, NDarray x1, NDarray @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                x2,
                x1,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("matmul", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Compute tensor dot product along specified axes for arrays &gt;= 1-D.
        /// 
        /// Given two tensors (arrays of dimension greater than or equal to one),
        /// a and b, and an array_like object containing two array_like
        /// objects, (a_axes, b_axes), sum the products of a’s and b’s
        /// elements (components) over the axes specified by a_axes and
        /// b_axes. The third argument can be a single non-negative
        /// integer_like scalar, N; if it is such, then the last N
        /// dimensions of a and the first N dimensions of b are summed
        /// over.
        /// 
        /// Notes
        /// 
        /// When axes is integer_like, the sequence for evaluation will be: first
        /// the -Nth axis in a and 0th axis in b, and the -1th axis in a and
        /// Nth axis in b last.
        /// 
        /// When there is more than one axis to sum over - and they are not the last
        /// (first) axes of a (b) - the argument axes should consist of
        /// two sequences of the same length, with the first axis to sum over given
        /// first in both sequences, the second axis second, and so forth.
        /// </summary>
        /// <param name="b">
        /// Tensors to “dot”.
        /// </param>
        /// <param name="a">
        /// Tensors to “dot”.
        /// </param>
        public NDarray tensordot(NDarray b, NDarray a, int[] axes = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                b,
                a,
                axes,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("tensordot", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Evaluates the Einstein summation convention on the operands.
        /// 
        /// Using the Einstein summation convention, many common multi-dimensional,
        /// linear algebraic array operations can be represented in a simple fashion.
        /// In implicit mode einsum computes these values.
        /// 
        /// In explicit mode, einsum provides further flexibility to compute
        /// other array operations that might not be considered classical Einstein
        /// summation operations, by disabling, or forcing summation over specified
        /// subscript labels.
        /// 
        /// See the notes and examples for clarification.
        /// 
        /// Notes
        /// 
        /// The Einstein summation convention can be used to compute
        /// many multi-dimensional, linear algebraic array operations. einsum
        /// provides a succinct way of representing these.
        /// 
        /// A non-exhaustive list of these operations,
        /// which can be computed by einsum, is shown below along with examples:
        /// 
        /// The subscripts string is a comma-separated list of subscript labels,
        /// where each label refers to a dimension of the corresponding operand.
        /// Whenever a label is repeated it is summed, so np.einsum('i,i', a, b)
        /// is equivalent to np.inner(a,b). If a label
        /// appears only once, it is not summed, so np.einsum('i', a) produces a
        /// view of a with no changes. A further example np.einsum('ij,jk', a, b)
        /// describes traditional matrix multiplication and is equivalent to
        /// np.matmul(a,b). Repeated subscript labels in one
        /// operand take the diagonal. For example, np.einsum('ii', a) is equivalent
        /// to np.trace(a).
        /// 
        /// In implicit mode, the chosen subscripts are important
        /// since the axes of the output are reordered alphabetically.  This
        /// means that np.einsum('ij', a) doesn’t affect a 2D array, while
        /// np.einsum('ji', a) takes its transpose. Additionally,
        /// np.einsum('ij,jk', a, b) returns a matrix multiplication, while,
        /// np.einsum('ij,jh', a, b) returns the transpose of the
        /// multiplication since subscript ‘h’ precedes subscript ‘i’.
        /// 
        /// In explicit mode the output can be directly controlled by
        /// specifying output subscript labels.  This requires the
        /// identifier ‘-&gt;’ as well as the list of output subscript labels.
        /// This feature increases the flexibility of the function since
        /// summing can be disabled or forced when required. The call
        /// np.einsum('i-&gt;', a) is like np.sum(a, axis=-1),
        /// and np.einsum('ii-&gt;i', a) is like np.diag(a).
        /// The difference is that einsum does not allow broadcasting by default.
        /// Additionally np.einsum('ij,jh-&gt;ih', a, b) directly specifies the
        /// order of the output subscript labels and therefore returns matrix
        /// multiplication, unlike the example above in implicit mode.
        /// 
        /// To enable and control broadcasting, use an ellipsis.  Default
        /// NumPy-style broadcasting is done by adding an ellipsis
        /// to the left of each term, like np.einsum('...ii-&gt;...i', a).
        /// To take the trace along the first and last axes,
        /// you can do np.einsum('i...i', a), or to do a matrix-matrix
        /// product with the left-most indices instead of rightmost, one can do
        /// np.einsum('ij...,jk...-&gt;ik...', a, b).
        /// 
        /// When there is only one operand, no axes are summed, and no output
        /// parameter is provided, a view into the operand is returned instead
        /// of a new array.  Thus, taking the diagonal as np.einsum('ii-&gt;i', a)
        /// produces a view (changed in version 1.10.0).
        /// 
        /// einsum also provides an alternative way to provide the subscripts
        /// and operands as einsum(op0, sublist0, op1, sublist1, ..., [sublistout]).
        /// If the output shape is not provided in this format einsum will be
        /// calculated in implicit mode, otherwise it will be performed explicitly.
        /// The examples below have corresponding einsum calls with the two
        /// parameter methods.
        /// 
        /// Views returned from einsum are now writeable whenever the input array
        /// is writeable. For example, np.einsum('ijk...-&gt;kji...', a) will now
        /// have the same effect as np.swapaxes(a, 0, 2)
        /// and np.einsum('ii-&gt;i', a) will return a writeable view of the diagonal
        /// of a 2D array.
        /// 
        /// Added the optimize argument which will optimize the contraction order
        /// of an einsum expression. For a contraction with three or more operands this
        /// can greatly increase the computational efficiency at the cost of a larger
        /// memory footprint during computation.
        /// 
        /// Typically a ‘greedy’ algorithm is applied which empirical tests have shown
        /// returns the optimal path in the majority of cases. In some cases ‘optimal’
        /// will return the superlative path through a more expensive, exhaustive search.
        /// For iterative calculations it may be advisable to calculate the optimal path
        /// once and reuse that path by supplying it as an argument. An example is given
        /// below.
        /// 
        /// See numpy.einsum_path for more details.
        /// </summary>
        /// <param name="subscripts">
        /// Specifies the subscripts for summation as comma separated list of
        /// subscript labels. An implicit (classical Einstein summation)
        /// calculation is performed unless the explicit indicator ‘-&gt;’ is
        /// included as well as subscript labels of the precise output form.
        /// </param>
        /// <param name="operands">
        /// These are the arrays for the operation.
        /// </param>
        /// <param name="@out">
        /// If provided, the calculation is done into this array.
        /// </param>
        /// <param name="dtype">
        /// If provided, forces the calculation to use the data type specified.
        /// Note that you may have to also give a more liberal casting
        /// parameter to allow the conversions. Default is None.
        /// </param>
        /// <param name="order">
        /// Controls the memory layout of the output. ‘C’ means it should
        /// be C contiguous. ‘F’ means it should be Fortran contiguous,
        /// ‘A’ means it should be ‘F’ if the inputs are all ‘F’, ‘C’ otherwise.
        /// ‘K’ means it should be as close to the layout as the inputs as
        /// is possible, including arbitrarily permuted axes.
        /// Default is ‘K’.
        /// </param>
        /// <param name="casting">
        /// Controls what kind of data casting may occur.  Setting this to
        /// ‘unsafe’ is not recommended, as it can adversely affect accumulations.
        /// 
        /// Default is ‘safe’.
        /// </param>
        /// <param name="optimize">
        /// Controls if intermediate optimization should occur. No optimization
        /// will occur if False and True will default to the ‘greedy’ algorithm.
        /// Also accepts an explicit contraction list from the np.einsum_path
        /// function. See np.einsum_path for more details. Defaults to False.
        /// </param>
        /// <returns>
        /// The calculation based on the Einstein summation convention.
        /// </returns>
        public NDarray einsum(string subscripts, NDarray[] operands, NDarray @out = null, Dtype dtype = null, string order = null, string casting = "safe", object optimize = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                subscripts,
                operands,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (order!=null) kwargs["order"]=ToPython(order);
            if (casting!="safe") kwargs["casting"]=ToPython(casting);
            if (optimize!=null) kwargs["optimize"]=ToPython(optimize);
            dynamic py = __self__.InvokeMethod("einsum", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /*
        /// <summary>
        /// Evaluates the lowest cost contraction order for an einsum expression by
        /// considering the creation of intermediate arrays.
        /// 
        /// Notes
        /// 
        /// The resulting path indicates which terms of the input contraction should be
        /// contracted first, the result of this contraction is then appended to the
        /// end of the contraction list. This list can then be iterated over until all
        /// intermediate contractions are complete.
        /// </summary>
        /// <param name="subscripts">
        /// Specifies the subscripts for summation.
        /// </param>
        /// <param name="operands">
        /// These are the arrays for the operation.
        /// </param>
        /// <param name="optimize">
        /// Choose the type of path. If a tuple is provided, the second argument is
        /// assumed to be the maximum intermediate size created. If only a single
        /// argument is provided the largest input or output array size is used
        /// as a maximum intermediate size.
        /// 
        /// Default is ‘greedy’.
        /// </param>
        /// <returns>
        /// A tuple of:
        /// path
        /// A list representation of the einsum path.
        /// string_repr
        /// A printable representation of the einsum path.
        /// </returns>
        public (list of tuples, string) einsum_path(string subscripts, NDarray[] operands, {bool optimize = "greedy")
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                subscripts,
                operands,
                optimize,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("einsum_path", pyargs, kwargs);
            var t = py as PyTuple;
            return (ToCsharp<list of tuples>(t[0]), ToCsharp<string>(t[1]));
        }
        */
        
        /// <summary>
        /// Raise a square matrix to the (integer) power n.
        /// 
        /// For positive integers n, the power is computed by repeated matrix
        /// squarings and matrix multiplications. If n == 0, the identity matrix
        /// of the same shape as M is returned. If n &lt; 0, the inverse
        /// is computed and then raised to the abs(n).
        /// </summary>
        /// <param name="a">
        /// Matrix to be “powered.”
        /// </param>
        /// <param name="n">
        /// The exponent can be any integer or long integer, positive,
        /// negative, or zero.
        /// </param>
        /// <returns>
        /// The return value is the same shape and type as M;
        /// if the exponent is positive or zero then the type of the
        /// elements is the same as those of M. If the exponent is
        /// negative the elements are floating-point.
        /// </returns>
        public NDarray matrix_power(NDarray a, int n)
        {
            //auto-generated code, do not change
            var linalg = self.GetAttr("linalg");
            var __self__=linalg;
            var pyargs=ToTuple(new object[]
            {
                a,
                n,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("matrix_power", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Kronecker product of two arrays.
        /// 
        /// Computes the Kronecker product, a composite array made of blocks of the
        /// second array scaled by the first.
        /// 
        /// Notes
        /// 
        /// The function assumes that the number of dimensions of a and b
        /// are the same, if necessary prepending the smallest with ones.
        /// If a.shape = (r0,r1,..,rN) and b.shape = (s0,s1,…,sN),
        /// the Kronecker product has shape (r0*s0, r1*s1, …, rN*SN).
        /// The elements are products of elements from a and b, organized
        /// explicitly by:
        /// 
        /// where:
        /// 
        /// In the common 2-D case (N=1), the block structure can be visualized:
        /// </summary>
        public NDarray kron(NDarray b, NDarray a)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                b,
                a,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("kron", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Compute the qr factorization of a matrix.
        /// 
        /// Factor the matrix a as qr, where q is orthonormal and r is
        /// upper-triangular.
        /// 
        /// Notes
        /// 
        /// This is an interface to the LAPACK routines dgeqrf, zgeqrf,
        /// dorgqr, and zungqr.
        /// 
        /// For more information on the qr factorization, see for example:
        /// https://en.wikipedia.org/wiki/QR_factorization
        /// 
        /// Subclasses of ndarray are preserved except for the ‘raw’ mode. So if
        /// a is of type matrix, all the return values will be matrices too.
        /// 
        /// New ‘reduced’, ‘complete’, and ‘raw’ options for mode were added in
        /// NumPy 1.8.0 and the old option ‘full’ was made an alias of ‘reduced’.  In
        /// addition the options ‘full’ and ‘economic’ were deprecated.  Because
        /// ‘full’ was the previous default and ‘reduced’ is the new default,
        /// backward compatibility can be maintained by letting mode default.
        /// The ‘raw’ option was added so that LAPACK routines that can multiply
        /// arrays by q using the Householder reflectors can be used. Note that in
        /// this case the returned arrays are of type np.double or np.cdouble and
        /// the h array is transposed to be FORTRAN compatible.  No routines using
        /// the ‘raw’ return are currently exposed by numpy, but some are available
        /// in lapack_lite and just await the necessary work.
        /// </summary>
        /// <param name="a">
        /// Matrix to be factored.
        /// </param>
        /// <param name="mode">
        /// If K = min(M, N), then
        /// 
        /// The options ‘reduced’, ‘complete, and ‘raw’ are new in numpy 1.8,
        /// see the notes for more information. The default is ‘reduced’, and to
        /// maintain backward compatibility with earlier versions of numpy both
        /// it and the old default ‘full’ can be omitted. Note that array h
        /// returned in ‘raw’ mode is transposed for calling Fortran. The
        /// ‘economic’ mode is deprecated.  The modes ‘full’ and ‘economic’ may
        /// be passed using only the first letter for backwards compatibility,
        /// but all others must be spelled out. See the Notes for more
        /// explanation.
        /// </param>
        /// <returns>
        /// A tuple of:
        /// q
        /// A matrix with orthonormal columns. When mode = ‘complete’ the
        /// result is an orthogonal/unitary matrix depending on whether or not
        /// a is real/complex. The determinant may be either +/- 1 in that
        /// case.
        /// r
        /// The upper-triangular matrix.
        /// (h, tau)
        /// The array h contains the Householder reflectors that generate q
        /// along with r. The tau array contains scaling factors for the
        /// reflectors. In the deprecated  ‘economic’ mode only h is returned.
        /// </returns>
        public (NDarray, NDarray, NDarray) qr(NDarray a, string mode = "reduced")
        {
            //auto-generated code, do not change
            var linalg = self.GetAttr("linalg");
            var __self__=linalg;
            var pyargs=ToTuple(new object[]
            {
                a,
            });
            var kwargs=new PyDict();
            if (mode!="reduced") kwargs["mode"]=ToPython(mode);
            dynamic py = __self__.InvokeMethod("qr", pyargs, kwargs);
            var t = py as PyTuple;
            return (ToCsharp<NDarray>(t[0]), ToCsharp<NDarray>(t[1]), ToCsharp<NDarray>(t[2]));
        }
        
        /*
        /// <summary>
        /// Compute the condition number of a matrix.
        /// 
        /// This function is capable of returning the condition number using
        /// one of seven different norms, depending on the value of p (see
        /// Parameters below).
        /// 
        /// Notes
        /// 
        /// The condition number of x is defined as the norm of x times the
        /// norm of the inverse of x [1]; the norm can be the usual L2-norm
        /// (root-of-sum-of-squares) or one of a number of other matrix norms.
        /// 
        /// References
        /// </summary>
        /// <param name="x">
        /// The matrix whose condition number is sought.
        /// </param>
        /// <param name="p">
        /// Order of the norm:
        /// 
        /// inf means the numpy.inf object, and the Frobenius norm is
        /// the root-of-sum-of-squares norm.
        /// </param>
        /// <returns>
        /// The condition number of the matrix. May be infinite.
        /// </returns>
        public {float cond(NDarray x, {None p = null)
        {
            //auto-generated code, do not change
            var linalg = self.GetAttr("linalg");
            var __self__=linalg;
            var pyargs=ToTuple(new object[]
            {
                x,
            });
            var kwargs=new PyDict();
            if (p!=null) kwargs["p"]=ToPython(p);
            dynamic py = __self__.InvokeMethod("cond", pyargs, kwargs);
            return ToCsharp<{float>(py);
        }
        */
        
        /// <summary>
        /// Return matrix rank of array using SVD method
        /// 
        /// Rank of the array is the number of singular values of the array that are
        /// greater than tol.
        /// 
        /// Notes
        /// 
        /// The default threshold to detect rank deficiency is a test on the magnitude
        /// of the singular values of M.  By default, we identify singular values less
        /// than S.max() * max(M.shape) * eps as indicating rank deficiency (with
        /// the symbols defined above). This is the algorithm MATLAB uses [1].  It also
        /// appears in Numerical recipes in the discussion of SVD solutions for linear
        /// least squares [2].
        /// 
        /// This default threshold is designed to detect rank deficiency accounting for
        /// the numerical errors of the SVD computation.  Imagine that there is a column
        /// in M that is an exact (in floating point) linear combination of other
        /// columns in M. Computing the SVD on M will not produce a singular value
        /// exactly equal to 0 in general: any difference of the smallest SVD value from
        /// 0 will be caused by numerical imprecision in the calculation of the SVD.
        /// Our threshold for small SVD values takes this numerical imprecision into
        /// account, and the default threshold will detect such numerical rank
        /// deficiency.  The threshold may declare a matrix M rank deficient even if
        /// the linear combination of some columns of M is not exactly equal to
        /// another column of M but only numerically very close to another column of
        /// M.
        /// 
        /// We chose our default threshold because it is in wide use.  Other thresholds
        /// are possible.  For example, elsewhere in the 2007 edition of Numerical
        /// recipes there is an alternative threshold of S.max() *
        /// np.finfo(M.dtype).eps / 2. * np.sqrt(m + n + 1.). The authors describe
        /// this threshold as being based on “expected roundoff error” (p 71).
        /// 
        /// The thresholds above deal with floating point roundoff error in the
        /// calculation of the SVD.  However, you may have more information about the
        /// sources of error in M that would make you consider other tolerance values
        /// to detect effective rank deficiency.  The most useful measure of the
        /// tolerance depends on the operations you intend to use on your matrix.  For
        /// example, if your data come from uncertain measurements with uncertainties
        /// greater than floating point epsilon, choosing a tolerance near that
        /// uncertainty may be preferable.  The tolerance may be absolute if the
        /// uncertainties are absolute rather than relative.
        /// 
        /// References
        /// </summary>
        /// <param name="M">
        /// input vector or stack of matrices
        /// </param>
        /// <param name="tol">
        /// threshold below which SVD values are considered zero. If tol is
        /// None, and S is an array with singular values for M, and
        /// eps is the epsilon value for datatype of S, then tol is
        /// set to S.max() * max(M.shape) * eps.
        /// </param>
        /// <param name="hermitian">
        /// If True, M is assumed to be Hermitian (symmetric if real-valued),
        /// enabling a more efficient method for finding singular values.
        /// Defaults to False.
        /// </param>
        public int matrix_rank(NDarray M, NDarray tol = null, bool? hermitian = false)
        {
            //auto-generated code, do not change
            var linalg = self.GetAttr("linalg");
            var __self__=linalg;
            var pyargs=ToTuple(new object[]
            {
                M,
            });
            var kwargs=new PyDict();
            if (tol!=null) kwargs["tol"]=ToPython(tol);
            if (hermitian!=false) kwargs["hermitian"]=ToPython(hermitian);
            dynamic py = __self__.InvokeMethod("matrix_rank", pyargs, kwargs);
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Compute the sign and (natural) logarithm of the determinant of an array.
        /// 
        /// If an array has a very small or very large determinant, then a call to
        /// det may overflow or underflow. This routine is more robust against such
        /// issues, because it computes the logarithm of the determinant rather than
        /// the determinant itself.
        /// 
        /// Notes
        /// 
        /// Broadcasting rules apply, see the numpy.linalg documentation for
        /// details.
        /// 
        /// The determinant is computed via LU factorization using the LAPACK
        /// routine z/dgetrf.
        /// </summary>
        /// <param name="a">
        /// Input array, has to be a square 2-D array.
        /// </param>
        /// <returns>
        /// A tuple of:
        /// sign
        /// A number representing the sign of the determinant. For a real matrix,
        /// this is 1, 0, or -1. For a complex matrix, this is a complex number
        /// with absolute value 1 (i.e., it is on the unit circle), or else 0.
        /// logdet
        /// The natural log of the absolute value of the determinant.
        /// </returns>
        public (NDarray, NDarray) slogdet(NDarray a)
        {
            //auto-generated code, do not change
            var linalg = self.GetAttr("linalg");
            var __self__=linalg;
            var pyargs=ToTuple(new object[]
            {
                a,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("slogdet", pyargs, kwargs);
            var t = py as PyTuple;
            return (ToCsharp<NDarray>(t[0]), ToCsharp<NDarray>(t[1]));
        }
        
        /// <summary>
        /// Return the sum along diagonals of the array.
        /// 
        /// If a is 2-D, the sum along its diagonal with the given offset
        /// is returned, i.e., the sum of elements a[i,i+offset] for all i.
        /// 
        /// If a has more than two dimensions, then the axes specified by axis1 and
        /// axis2 are used to determine the 2-D sub-arrays whose traces are returned.
        /// The shape of the resulting array is the same as that of a with axis1
        /// and axis2 removed.
        /// </summary>
        /// <param name="a">
        /// Input array, from which the diagonals are taken.
        /// </param>
        /// <param name="offset">
        /// Offset of the diagonal from the main diagonal. Can be both positive
        /// and negative. Defaults to 0.
        /// </param>
        /// <param name="axis2">
        /// Axes to be used as the first and second axis of the 2-D sub-arrays
        /// from which the diagonals should be taken. Defaults are the first two
        /// axes of a.
        /// </param>
        /// <param name="axis1">
        /// Axes to be used as the first and second axis of the 2-D sub-arrays
        /// from which the diagonals should be taken. Defaults are the first two
        /// axes of a.
        /// </param>
        /// <param name="dtype">
        /// Determines the data-type of the returned array and of the accumulator
        /// where the elements are summed. If dtype has the value None and a is
        /// of integer type of precision less than the default integer
        /// precision, then the default integer precision is used. Otherwise,
        /// the precision is the same as that of a.
        /// </param>
        /// <param name="@out">
        /// Array into which the output is placed. Its type is preserved and
        /// it must be of the right shape to hold the output.
        /// </param>
        /// <returns>
        /// If a is 2-D, the sum along the diagonal is returned.  If a has
        /// larger dimensions, then an array of sums along diagonals is returned.
        /// </returns>
        public NDarray trace(NDarray a, int? offset = 0, int? axis2 = null, int? axis1 = null, Dtype dtype = null, NDarray @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                a,
            });
            var kwargs=new PyDict();
            if (offset!=0) kwargs["offset"]=ToPython(offset);
            if (axis2!=null) kwargs["axis2"]=ToPython(axis2);
            if (axis1!=null) kwargs["axis1"]=ToPython(axis1);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("trace", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Solve the tensor equation a x = b for x.
        /// 
        /// It is assumed that all indices of x are summed over in the product,
        /// together with the rightmost indices of a, as is done in, for example,
        /// tensordot(a, x, axes=b.ndim).
        /// </summary>
        /// <param name="a">
        /// Coefficient tensor, of shape b.shape + Q. Q, a tuple, equals
        /// the shape of that sub-tensor of a consisting of the appropriate
        /// number of its rightmost indices, and must be such that
        /// prod(Q) == prod(b.shape) (in which sense a is said to be
        /// ‘square’).
        /// </param>
        /// <param name="b">
        /// Right-hand tensor, which can be of any shape.
        /// </param>
        /// <param name="axes">
        /// Axes in a to reorder to the right, before inversion.
        /// If None (default), no reordering is done.
        /// </param>
        public NDarray tensorsolve(NDarray a, NDarray b, int[] axes = null)
        {
            //auto-generated code, do not change
            var linalg = self.GetAttr("linalg");
            var __self__=linalg;
            var pyargs=ToTuple(new object[]
            {
                a,
                b,
            });
            var kwargs=new PyDict();
            if (axes!=null) kwargs["axes"]=ToPython(axes);
            dynamic py = __self__.InvokeMethod("tensorsolve", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Compute the ‘inverse’ of an N-dimensional array.
        /// 
        /// The result is an inverse for a relative to the tensordot operation
        /// tensordot(a, b, ind), i. e., up to floating-point accuracy,
        /// tensordot(tensorinv(a), a, ind) is the “identity” tensor for the
        /// tensordot operation.
        /// </summary>
        /// <param name="a">
        /// Tensor to ‘invert’. Its shape must be ‘square’, i. e.,
        /// prod(a.shape[:ind]) == prod(a.shape[ind:]).
        /// </param>
        /// <param name="ind">
        /// Number of first indices that are involved in the inverse sum.
        /// Must be a positive integer, default is 2.
        /// </param>
        /// <returns>
        /// a’s tensordot inverse, shape a.shape[ind:] + a.shape[:ind].
        /// </returns>
        public NDarray tensorinv(NDarray a, int? ind = 2)
        {
            //auto-generated code, do not change
            var linalg = self.GetAttr("linalg");
            var __self__=linalg;
            var pyargs=ToTuple(new object[]
            {
                a,
            });
            var kwargs=new PyDict();
            if (ind!=2) kwargs["ind"]=ToPython(ind);
            dynamic py = __self__.InvokeMethod("tensorinv", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Generic Python-exception-derived object raised by linalg functions.
        /// 
        /// General purpose exception class, derived from Python’s exception.Exception
        /// class, programmatically raised in linalg functions when a Linear
        /// Algebra-related condition would prevent further correct execution of the
        /// function.
        /// </summary>
        public void LinAlgError()
        {
            //auto-generated code, do not change
            var linalg = self.GetAttr("linalg");
            var __self__=linalg;
            dynamic py = __self__.InvokeMethod("LinAlgError");
        }
        
    }
}
